{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers architecture and BERT\n",
    "<sup>This notebook is a part of Natural Language Processing class at the University of Ljubljana, Faculty for computer and information science. Please contact [slavko.zitnik@fri.uni-lj.si](mailto:slavko.zitnik@fri.uni-lj.si) for any comments.</sub>\n",
    "\n",
    "[Transformers](https://huggingface.co/transformers/quicktour.html) library offers a variety of implemented architectures (Tensorflow and PyTorch) along with [pre-trained models](https://huggingface.co/models) for different tasks - sequence classification, sequence tagging, machine translation, .... There you can find also some Slovene models. Otherwise, Slovene models are available at:\n",
    "   \n",
    "* [CroSloEn BERT](https://www.clarin.si/repository/xmlui/handle/11356/1330)\n",
    "* [SloBERTa 1.0](https://www.clarin.si/repository/xmlui/handle/11356/1387)\n",
    "* [SloBERTa 2.0](https://www.clarin.si/repository/xmlui/handle/11356/1397)\n",
    "\n",
    "[A nice introduction into BERT](https://huggingface.co/blog/bert-101) (for reading).\n",
    "\n",
    "\n",
    "The examples here require at least >4GB GPU (adapt batch sizes for smaller cards) and Tensorflow 2.x library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.4.1\n",
      "The system contains '1' Physical GPUs and '1' Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "print(f\"Tensorflow version: {tf.__version__}\")\n",
    "\n",
    "# Restrict TensorFlow to only allocate 4GBs of memory on the first GPU\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=4096)])\n",
    "    #tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(f\"The system contains '{len(gpus)}' Physical GPUs and '{len(logical_gpus)}' Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    print(e)\n",
    "else:\n",
    "    print(f\"Your system does not contain a GPU that could be used by Tensorflow!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8lpm8TuqGMs1"
   },
   "source": [
    "We import general libraries that will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hm3lCq7uGM1a"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import (TFBertForSequenceClassification, \n",
    "                          BertTokenizer)\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the dataset and change the sentiment values to number format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MvkQrdEbGM91"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  One of the other reviewers has mentioned that ...          1\n",
       "1  A wonderful little production. <br /><br />The...          1\n",
       "2  I thought this was a wonderful way to spend ti...          1\n",
       "3  Basically there's a family where a little boy ...          0\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...          1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('IMDB Dataset.csv')\n",
    "\n",
    "# Transform positive/negative values to 1/0s\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "data['sentiment'] = label_encoder.fit_transform(data['sentiment'])\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UvcC8y4aGNAs"
   },
   "source": [
    "Split the data into train, development and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VmHH2mVUGNDm"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset shape: (40000,), \n",
      "Test dataset shape: (5000,) \n",
      "Validation dataset shape: (5000,)\n"
     ]
    }
   ],
   "source": [
    "X = (np.array(data['review']))\n",
    "y = (np.array(data['sentiment']))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=13)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42)\n",
    "print(\"Train dataset shape: {0}, \\nTest dataset shape: {1} \\nValidation dataset shape: {2}\".format(X_train.shape, X_test.shape, X_val.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the models from the public transformers repository. Generally for each model we load the classifier (i.e. trained model w/o specific head) and tokenizer. Tokenizer is used to transform input into tokens and word parts that can be fed to the classifier based on the token id in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yg978u_iGNJa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some layers of TFBertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bert_model = TFBertForSequenceClassification.from_pretrained(\"bert-base-cased\")\n",
    "#bert_model = TFBertForSequenceClassification.from_pretrained('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['don', \"'\", 't', 'be', 'so', 'judgment', '##al']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_tokenizer.tokenize(\"don't be so judgmental\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We prepare the input for the classifier (semi-manually using *encode_plus* method):\n",
    "\n",
    "* *input_ids* contain id of each token from the tokenizer vocabulary\n",
    "* *attention_masks* identify which is used to avoid using attention mechanism on padded tokens\n",
    "* *token_type_ids* represent the sequence part of the input (used during pre-training for next sentence prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KQfWAE_MTJxk"
   },
   "outputs": [],
   "source": [
    "pad_token=0\n",
    "pad_token_segment_id=0\n",
    "max_length=128\n",
    "\n",
    "def convert_to_input(reviews):\n",
    "  input_ids,attention_masks,token_type_ids=[],[],[]\n",
    "  \n",
    "  for x in tqdm(reviews,position=0, leave=True):\n",
    "    inputs = bert_tokenizer.encode_plus(x,add_special_tokens=True, max_length=max_length)\n",
    "    \n",
    "    i, t = inputs[\"input_ids\"], inputs[\"token_type_ids\"]\n",
    "    m = [1] * len(i)\n",
    "\n",
    "    padding_length = max_length - len(i)\n",
    "\n",
    "    i = i + ([pad_token] * padding_length)\n",
    "    m = m + ([0] * padding_length)\n",
    "    t = t + ([pad_token_segment_id] * padding_length)\n",
    "    \n",
    "    input_ids.append(i)\n",
    "    attention_masks.append(m)\n",
    "    token_type_ids.append(t)\n",
    "  \n",
    "  return [np.asarray(input_ids), \n",
    "            np.asarray(attention_masks), \n",
    "            np.asarray(token_type_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "kitY8ZX4mHwe",
    "outputId": "bfa8d76f-2c63-4860-b789-7c054a43f6bf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:00<?, ?it/s]Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "100%|██████████| 5000/5000 [00:20<00:00, 246.71it/s]\n",
      "100%|██████████| 40000/40000 [02:41<00:00, 247.97it/s]\n",
      "100%|██████████| 5000/5000 [00:19<00:00, 252.81it/s]\n"
     ]
    }
   ],
   "source": [
    "X_test_input=convert_to_input(X_test)\n",
    "X_train_input=convert_to_input(X_train)\n",
    "X_val_input=convert_to_input(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow models by default take object of type [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) as input for training or prediction. It allows for shuffling, splitting or automatic batch creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2--NOpQk7dMg"
   },
   "outputs": [],
   "source": [
    "def example_to_features(input_ids,attention_masks,token_type_ids,y):\n",
    "  return {\"input_ids\": input_ids,\n",
    "          \"attention_mask\": attention_masks,\n",
    "          \"token_type_ids\": token_type_ids},y\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices((X_train_input[0],X_train_input[1],X_train_input[2],y_train)).map(example_to_features).shuffle(100).batch(12).repeat(5)\n",
    "val_ds=tf.data.Dataset.from_tensor_slices((X_val_input[0],X_val_input[1],X_val_input[2],y_val)).map(example_to_features).batch(12)\n",
    "test_ds=tf.data.Dataset.from_tensor_slices((X_test_input[0],X_test_input[1],X_test_input[2],y_test)).map(example_to_features).batch(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We set the parameters ffor training and train the model. As our model is already pretrained and contains a specific head for sequence classification, we can use it directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bvZVfO_CGNgd"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5, epsilon=1e-08, clipnorm=1.0)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "\n",
    "bert_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "Qu2BiuepGNre",
    "outputId": "28d01ecb-ff88-4b1d-8ae2-ab7eb6c060b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning BERT on IMDB dataset\n",
      "Epoch 1/3\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fcca8024a08>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7fcca6fed750> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fcca8024a08>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: <cyfunction Socket.send at 0x7fcca6fed750> is not a module, class, method, function, traceback, frame, or code object\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "16670/16670 [==============================] - ETA: 0s - loss: 0.2496 - accuracy: 0.8926WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "16670/16670 [==============================] - 5375s 321ms/step - loss: 0.2495 - accuracy: 0.8926 - val_loss: 0.6614 - val_accuracy: 0.8702\n",
      "Epoch 2/3\n",
      "16670/16670 [==============================] - 5371s 322ms/step - loss: 0.0426 - accuracy: 0.9865 - val_loss: 0.6901 - val_accuracy: 0.8692\n",
      "Epoch 3/3\n",
      "16670/16670 [==============================] - 5374s 322ms/step - loss: 0.0224 - accuracy: 0.9929 - val_loss: 1.1166 - val_accuracy: 0.8584\n"
     ]
    }
   ],
   "source": [
    "print(\"Fine-tuning BERT on IMDB dataset\")\n",
    "bert_history = bert_model.fit(train_ds, epochs=3, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuning will output something similar to the following:\n",
    "\n",
    "```\n",
    "Fine-tuning BERT on IMDB dataset\n",
    "Train for 16670 steps, validate for 417 steps\n",
    "Epoch 1/3\n",
    "16670/16670 [==] - 5116s 307ms/step - loss: 0.1515 - accuracy: 0.9392 - val_loss: 0.5599 - val_accuracy: 0.8676\n",
    "Epoch 2/3\n",
    "16670/16670 [==] - 5123s 307ms/step - loss: 0.0347 - accuracy: 0.9884 - val_loss: 0.4681 - val_accuracy: 0.8742\n",
    "Epoch 3/3\n",
    "16670/16670 [==] - 5136s 308ms/step - loss: 0.0254 - accuracy: 0.9920 - val_loss: 0.6523 - val_accuracy: 0.8668\n",
    "```\n",
    "\n",
    "We can observe that the loss is decreasing and the accuracy on the validation data is increasing. \n",
    "\n",
    "After training for a few epochs we evaluate the model against the test data. First we prepare true values as a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 1 ... 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "results_true = test_ds.unbatch()\n",
    "results_true = np.asarray([element[1].numpy() for element in results_true])\n",
    "print(results_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we get predictions from the model. As the predictions consist of vectors of dimension two, we select the final prediction class by selection the maximum value for the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
      "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
      "Model predictions:\n",
      " [[ 5.638682  -4.606999 ]\n",
      " [ 6.5738583 -5.4174294]\n",
      " [-6.3349085  7.2059326]\n",
      " ...\n",
      " [-5.756212   6.591787 ]\n",
      " [ 5.4666157 -4.4481173]\n",
      " [ 5.9342785 -4.881941 ]]\n"
     ]
    }
   ],
   "source": [
    "results = bert_model.predict(test_ds)\n",
    "print(f\"Model predictions:\\n {results.logits}\")\n",
    "\n",
    "results_predicted = np.argmax(results.logits, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we evaluate the model using standard scores such as F score and accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.8744437995743858\n",
      "Accuracy score: 0.8702\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f\"F1 score: {f1_score(results_true, results_predicted)}\")\n",
    "print(f\"Accuracy score: {accuracy_score(results_true, results_predicted)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results we achieve using such approach should be something like as follows:\n",
    "\n",
    "```\n",
    "F1 score: 0.88\n",
    "Accuracy score: 0.8788\n",
    "```\n",
    "\n",
    "We can save the fine-tuned model and then load it using the same approach as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at ./ were not used when initializing TFBertForSequenceClassification: ['dropout_37']\n",
      "- This IS expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at ./.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# SAVING YOUR MODEL\n",
    "bert_model.save_pretrained('./')\n",
    "\n",
    "# LOADING YOUR MODEL\n",
    "bert_model = TFBertForSequenceClassification.from_pretrained('./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom neural model for IMDB reviews sentiment prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's use additional parameters of the *encode_plus* function to achieve the same as above to get the *input_ids*. The input for our models will be just indices of words and class values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slavkoz/anaconda3/envs/nlp-course-fri/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n"
     ]
    }
   ],
   "source": [
    "def get_token_ids(texts):\n",
    "    return bert_tokenizer.batch_encode_plus(texts, \n",
    "                                            add_special_tokens=True, \n",
    "                                            max_length = 128, \n",
    "                                            pad_to_max_length = True)[\"input_ids\"]\n",
    "\n",
    "train_token_ids = get_token_ids(X_train)\n",
    "test_token_ids = get_token_ids(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((tf.constant(train_token_ids), tf.constant(y_train))).batch(12)\n",
    "test_data = tf.data.Dataset.from_tensor_slices((tf.constant(test_token_ids), tf.constant(y_test))).batch(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Tensorflow API allows for [custom model creation](https://www.tensorflow.org/api_docs/python/tf/keras/Model) and [inclusion of existing models/layers](https://www.tensorflow.org/guide/keras/custom_layers_and_models) into a model. We create a custom model using several layers as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class CustomIMDBModel(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocabulary_size,\n",
    "                 embedding_dimensions=128,\n",
    "                 cnn_filters=50,\n",
    "                 dnn_units=512,\n",
    "                 model_output_classes=2,\n",
    "                 dropout_rate=0.1,\n",
    "                 training=False,\n",
    "                 name=\"custom_imdb_model\"):\n",
    "        super(CustomIMDBModel, self).__init__(name=name)\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocabulary_size,\n",
    "                                          embedding_dimensions)\n",
    "        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=2,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=3,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=4,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "        \n",
    "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        if model_output_classes == 2:\n",
    "            self.last_dense = layers.Dense(units=1,\n",
    "                                           activation=\"sigmoid\")\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units=model_output_classes,\n",
    "                                           activation=\"softmax\")\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        l = self.embedding(inputs)\n",
    "        l_1 = self.cnn_layer1(l) \n",
    "        l_1 = self.pool(l_1) \n",
    "        l_2 = self.cnn_layer2(l) \n",
    "        l_2 = self.pool(l_2)\n",
    "        l_3 = self.cnn_layer3(l)\n",
    "        l_3 = self.pool(l_3) \n",
    "        \n",
    "        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n",
    "        concatenated = self.dense_1(concatenated)\n",
    "        concatenated = self.dropout(concatenated, training)\n",
    "        model_output = self.last_dense(concatenated)\n",
    "        \n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_LENGTH = len(bert_tokenizer.vocab)\n",
    "EMB_DIM = 200\n",
    "CNN_FILTERS = 100\n",
    "DNN_UNITS = 256\n",
    "OUTPUT_CLASSES = 2\n",
    "DROPOUT_RATE = 0.2\n",
    "NB_EPOCHS = 5\n",
    "\n",
    "custom_model = CustomIMDBModel(vocabulary_size=VOCAB_LENGTH,\n",
    "                        embedding_dimensions=EMB_DIM,\n",
    "                        cnn_filters=CNN_FILTERS,\n",
    "                        dnn_units=DNN_UNITS,\n",
    "                        model_output_classes=OUTPUT_CLASSES,\n",
    "                        dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OUTPUT_CLASSES == 2:\n",
    "    custom_model.compile(loss=\"binary_crossentropy\",\n",
    "                       optimizer=\"adam\",\n",
    "                       metrics=[\"accuracy\"])\n",
    "else:\n",
    "    custom_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                       optimizer=\"adam\",\n",
    "                       metrics=[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3334/3334 [==============================] - 105s 31ms/step - loss: 0.4966 - accuracy: 0.7442\n",
      "Epoch 2/5\n",
      "3334/3334 [==============================] - 101s 30ms/step - loss: 0.2715 - accuracy: 0.8860\n",
      "Epoch 3/5\n",
      "3334/3334 [==============================] - 100s 30ms/step - loss: 0.1116 - accuracy: 0.9604\n",
      "Epoch 4/5\n",
      "3334/3334 [==============================] - 100s 30ms/step - loss: 0.0628 - accuracy: 0.9767\n",
      "Epoch 5/5\n",
      "3334/3334 [==============================] - 100s 30ms/step - loss: 0.0358 - accuracy: 0.9869\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcaa4feb1d0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_model.fit(train_data, epochs=NB_EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fine-tuning will output something similar to the following:\n",
    "\n",
    "```\n",
    "Train for 3334 steps\n",
    "Epoch 1/5\n",
    "3334/3334 [==] - 123s 37ms/step - loss: 0.4285 - accuracy: 0.7985\n",
    "Epoch 2/5\n",
    "3334/3334 [==] - 121s 36ms/step - loss: 0.2344 - accuracy: 0.9065\n",
    "Epoch 3/5\n",
    "3334/3334 [==] - 121s 36ms/step - loss: 0.0862 - accuracy: 0.9693\n",
    "Epoch 4/5\n",
    "3334/3334 [==] - 121s 36ms/step - loss: 0.0537 - accuracy: 0.9805\n",
    "Epoch 5/5\n",
    "3334/3334 [==] - 123s 37ms/step - loss: 0.0355 - accuracy: 0.9874\n",
    "```\n",
    "\n",
    "We can observe that the loss is decreasing and the accuracy on the validation data is increasing. \n",
    "\n",
    "After training for a few epochs we evaluate the model against the test data. First we prepare true values as a numpy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_predicted = [1 if x>=0.5 else 0 for x in custom_model.predict(test_data) ]\n",
    "results_true = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.8379888268156425\n",
      "Accuracy score: 0.826\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f\"F1 score: {f1_score(results_true, results_predicted)}\")\n",
    "print(f\"Accuracy score: {accuracy_score(results_true, results_predicted)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results we achieve using such approach should be something like as follows:\n",
    "\n",
    "```\n",
    "F1 score: 0.84\n",
    "Accuracy score: 0.83\n",
    "```\n",
    "\n",
    "The model achieves decent performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom neural model for IMDB reviews sentiment prediction using BERT Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has been shown that BERT embeddings can be used to improve your models (see lecture materials). In this scenario we change the custom model above and use BERT embeddings as input for further models.\n",
    "\n",
    "We load the plain [TFBert model](https://huggingface.co/docs/transformers/model_doc/bert#transformers.TFBertModel) without a specific head and use the last layer as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[-0.53795666  0.02915638  0.4568426  ... -0.4206041   0.3360427\n",
      "   -0.68869185]\n",
      "  [-0.02613155  0.10147773  0.38023055 ...  0.16423033  0.6685274\n",
      "   -0.07883158]\n",
      "  [-0.8667504   0.8550649   0.72283715 ... -0.56866455  0.08689685\n",
      "    0.24533442]\n",
      "  ...\n",
      "  [ 0.3817766  -0.4433983   0.7945325  ... -0.62353337 -0.00921943\n",
      "   -1.3112266 ]\n",
      "  [ 0.33190864 -0.4060634   0.8477452  ... -0.6768912  -0.03242041\n",
      "   -1.2069647 ]\n",
      "  [ 0.3906283  -0.44234878  0.72817904 ... -0.66963124 -0.02545467\n",
      "   -1.3906384 ]]], shape=(1, 55, 768), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/slavkoz/anaconda3/envs/nlp-course-fri/lib/python3.6/site-packages/transformers-4.4.2-py3.8.egg/transformers/tokenization_utils_base.py:2074: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertModel\n",
    "import tensorflow as tf\n",
    "\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "token_ids = bert_tokenizer.encode(\"Hello, my dog is cute\", add_special_tokens=True, max_length = 55, pad_to_max_length = True)\n",
    "input_ids = tf.constant(token_ids)[None, :]  # Batch size 1\n",
    "outputs = bert_model(input_ids)\n",
    "last_hidden_states = outputs[0]\n",
    "print(last_hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_ids(texts):\n",
    "    return bert_tokenizer.batch_encode_plus(texts, \n",
    "                                            add_special_tokens=True, \n",
    "                                            max_length = 128, \n",
    "                                            pad_to_max_length = True)[\"input_ids\"]\n",
    "\n",
    "train_token_ids = get_token_ids(X_train)\n",
    "test_token_ids = get_token_ids(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = tf.data.Dataset.from_tensor_slices((tf.constant(train_token_ids), tf.constant(y_train))).batch(12)\n",
    "test_data = tf.data.Dataset.from_tensor_slices((tf.constant(test_token_ids), tf.constant(y_test))).batch(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom model improvement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, TFBertModel, TFBertPreTrainedModel, TFBertMainLayer\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import tensorflow as tf\n",
    "class BertIMDBEmbeddingModel(TFBertPreTrainedModel):\n",
    "    def __init__(self, config,\n",
    "                 cnn_filters=50,\n",
    "                 dnn_units=512,\n",
    "                 model_output_classes=2,\n",
    "                 dropout_rate=0.1,\n",
    "                 training=False,\n",
    "                 name=\"text_model\",\n",
    "                 *inputs, **kwargs):\n",
    "        super().__init__(config, *inputs, **kwargs)\n",
    "        self.bert = TFBertMainLayer(config, name=\"bert\", trainable = False)\n",
    "        \n",
    "        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=2,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=3,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=4,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "        \n",
    "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        if model_output_classes == 2:\n",
    "            self.last_dense = layers.Dense(units=1,\n",
    "                                           activation=\"sigmoid\")\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units=model_output_classes,\n",
    "                                           activation=\"softmax\")\n",
    "\n",
    "    def call(self, inputs, training = False, **kwargs):        \n",
    "        bert_outputs = self.bert(inputs, training = training, **kwargs)\n",
    "        \n",
    "        l_1 = self.cnn_layer1(bert_outputs[0]) \n",
    "        l_1 = self.pool(l_1) \n",
    "        l_2 = self.cnn_layer2(bert_outputs[0]) \n",
    "        l_2 = self.pool(l_2)\n",
    "        l_3 = self.cnn_layer3(bert_outputs[0])\n",
    "        l_3 = self.pool(l_3) \n",
    "        \n",
    "        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n",
    "        concatenated = self.dense_1(concatenated)\n",
    "        concatenated = self.dropout(concatenated, training)\n",
    "        model_output = self.last_dense(concatenated)\n",
    "        \n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading and training our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing BertIMDBEmbeddingModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing BertIMDBEmbeddingModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertIMDBEmbeddingModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of BertIMDBEmbeddingModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['conv1d_5', 'dense_3', 'global_max_pooling1d_1', 'dropout_151', 'conv1d_4', 'conv1d_3', 'dense_2']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "CNN_FILTERS = 100\n",
    "DNN_UNITS = 256\n",
    "OUTPUT_CLASSES = 2\n",
    "DROPOUT_RATE = 0.2\n",
    "NB_EPOCHS = 5\n",
    "\n",
    "text_model = BertIMDBEmbeddingModel.from_pretrained('bert-base-uncased',\n",
    "                        cnn_filters=CNN_FILTERS,\n",
    "                        dnn_units=DNN_UNITS,\n",
    "                        model_output_classes=OUTPUT_CLASSES,\n",
    "                        dropout_rate=DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OUTPUT_CLASSES == 2:\n",
    "    text_model.compile(loss=\"binary_crossentropy\",\n",
    "                       optimizer=\"adam\",\n",
    "                       metrics=[\"accuracy\"])\n",
    "else:\n",
    "    text_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                       optimizer=\"adam\",\n",
    "                       metrics=[\"sparse_categorical_accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3334/3334 [==============================] - 392s 115ms/step - loss: 0.4461 - accuracy: 0.7969\n",
      "Epoch 2/5\n",
      "3334/3334 [==============================] - 391s 117ms/step - loss: 0.3224 - accuracy: 0.8632\n",
      "Epoch 3/5\n",
      "3334/3334 [==============================] - 391s 117ms/step - loss: 0.2866 - accuracy: 0.8807\n",
      "Epoch 4/5\n",
      "3334/3334 [==============================] - 391s 117ms/step - loss: 0.2569 - accuracy: 0.8921\n",
      "Epoch 5/5\n",
      "3334/3334 [==============================] - 392s 117ms/step - loss: 0.2228 - accuracy: 0.9104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fcb3c22c748>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_model.fit(train_data, epochs=NB_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_predicted = [1 if x>=0.5 else 0 for x in text_model.predict(test_data) ]\n",
    "results_true = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.8747278844250941\n",
      "Accuracy score: 0.8734\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(f\"F1 score: {f1_score(results_true, results_predicted)}\")\n",
    "print(f\"Accuracy score: {accuracy_score(results_true, results_predicted)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results we achieve using such approach should be something like as follows:\n",
    "\n",
    "```\n",
    "F1 score: 0.88\n",
    "Accuracy score: 0.88\n",
    "```\n",
    "\n",
    "We achieve better performance than plain custom model with simple embeddings but a bit lower performance than the fine-tuned model (probably not significantly different due to hyperparameters modification).\n",
    "\n",
    "By setting the parameter *trainable = False* in a model layer, we can disable model weights updating (i.e. fine-tuning). If we enable weights updating for the whole BERT model in the example above, we get much worse performance as there are many more parameters to update. To be sure which parameters are updating, use *model.summary* function to output the architecture and parameters of your model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "    \n",
    "* [BERT scientific paper explained](http://nlp.seas.harvard.edu/2018/04/03/attention.html)\n",
    "* [High-level explanation of BERT](https://towardsdatascience.com/lost-in-translation-found-by-transformer-46a16bf6418f)\n",
    "* [Sequence models vs. BERT](https://medium.com/saarthi-ai/transformers-attention-based-seq2seq-machine-translation-a28940aaa4fe) and an [opinion](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/) of why BERT achieves better performance.\n",
    "* [Movie reviews using TF 2.0](https://androidkt.com/state-of-the-art-text-classification-using-bert-in-ten-lines-of-tensorflow-2-0) \n",
    "* [Movie reviews using BERT embeddings](https://stackabuse.com/text-classification-with-bert-tokenizer-and-tf-2-0-in-python)\n",
    "* [BERT movie reviews notebook (TF 1x)](https://colab.research.google.com/github/google-research/bert/blob/master/predicting_movie_reviews_with_bert_on_tf_hub.ipynb#scrollTo=LL5W8gEGRTAf)\n",
    "* [BERT embeddings using TF 2.0](https://colab.research.google.com/drive/1hMLd5-r82FrnFnBub-B-fVW78Px4KPX1)\n",
    "* [Sentence-level embeddings](https://towardsdatascience.com/simple-bert-using-tensorflow-2-0-132cb19e9b22)\n",
    "* [FineTuning BERT, Named entity recognition using BERT](https://medium.com/swlh/named-entity-recognition-using-bert-2fb924864d47)\n",
    "* [DeepPavlov BERT models](http://docs.deeppavlov.ai/en/master/features/models/bert.html): BERT models for Slavic languages.\n",
    "* [LSTM and BERT example in PyTorch for classification](https://towardsdatascience.com/bert-for-dummies-step-by-step-tutorial-fb90890ffe03)\n",
    "* [LSTM in Keras for intent classification (NER-like)](https://towardsdatascience.com/natural-language-understanding-with-sequence-to-sequence-models-e87d41ad258b)\n",
    "* [GPT-2 for sequence classification](https://gmihaila.medium.com/gpt2-for-text-classification-using-hugging-face-transformers-574555451832)\n",
    "    \n",
    "   "
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT Text Classification IMDB.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:nlp-course-fri]",
   "language": "python",
   "name": "conda-env-nlp-course-fri-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
