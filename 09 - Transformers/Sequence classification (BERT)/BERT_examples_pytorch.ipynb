{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers architecture and BERT\n",
    "<sup>This notebook is a part of Natural Language Processing class at the University of Ljubljana, Faculty for computer and information science. Please contact [ales.zagar@fri.uni-lj.si](mailto:ales.zagar@fri.uni-lj.si) for any comments.</sub>\n",
    "\n",
    "[Transformers](https://huggingface.co/transformers/quicktour.html) library offers a variety of implemented architectures (Tensorflow and PyTorch) along with [pre-trained models](https://huggingface.co/models) for different tasks - sequence classification, sequence tagging, machine translation, .... There you can find also some Slovene models. Otherwise, Slovene models are available at:\n",
    "   \n",
    "* [CroSloEn BERT](https://www.clarin.si/repository/xmlui/handle/11356/1330)\n",
    "* [SloBERTa 1.0](https://www.clarin.si/repository/xmlui/handle/11356/1387)\n",
    "* [SloBERTa 2.0](https://www.clarin.si/repository/xmlui/handle/11356/1397)\n",
    "\n",
    "[A nice introduction into BERT](https://huggingface.co/blog/bert-101) (for reading).\n",
    "\n",
    "\n",
    "A lot of [notebooks](https://huggingface.co/docs/transformers/v4.39.1/en/notebooks) exist that can help you learn or speed up the model building process. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We read the dataset from the dataset library. We will be working with sentiment analysis task, labeling positive and negative movie reviews. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T15:19:08.497850Z",
     "start_time": "2024-03-30T15:18:52.728049Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "MvkQrdEbGM91"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.',\n",
       " 'label': 0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "imdb = load_dataset(\"imdb\")\n",
    "\n",
    "imdb['train'][0]  # print first example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenizers Overview\n",
    "\n",
    "Tokenizers play a fundamental role in Natural Language Processing (NLP) by breaking down text into smaller, more manageable units called tokens. These tokens can be words, subwords, or even characters, depending on the granularity required for the task at hand. The choice of tokenizer can significantly impact the performance of NLP models, as it affects how the text is represented and understood by the algorithms.\n",
    "\n",
    "## Types of Tokenizers\n",
    "\n",
    "1. **Word Tokenizers**: These tokenizers split text into words, using spaces and punctuation as delimiters. They are simple to implement and understand, but might not be effective for languages that don't use spaces to separate words, or for handling compound words in languages like German.\n",
    "\n",
    "2. **Subword Tokenizers**: Subword tokenization algorithms like Byte-Pair Encoding (BPE), WordPiece, and SentencePiece break words down into smaller units (subwords). This approach helps in handling out-of-vocabulary words, and provides a balance between the flexibility of character tokenization and the efficiency of word tokenization.\n",
    "\n",
    "3. **Character Tokenizers**: These tokenizers break text down to the character level, offering the highest granularity. This can be useful for tasks like character-level language modeling or languages with no clear word boundaries, but it usually leads to longer sequences compared to word or subword tokenization.\n",
    "\n",
    "4. **Byte-Level Tokenizers**: Similar to character tokenizers, byte-level tokenizers operate at the byte level, encoding each byte of the text as a separate token. This approach is language-agnostic and can handle any text without the need for a predefined vocabulary.\n",
    "\n",
    "## Importance in NLP\n",
    "\n",
    "Tokenization is the first step in preprocessing text data for NLP tasks. A well-chosen tokenizer can:\n",
    "\n",
    "- Improve model understanding of language nuances\n",
    "- Reduce the size of the vocabulary, leading to more efficient training and inference\n",
    "- Handle a wide range of languages and special text elements like emojis or domain-specific terms\n",
    "\n",
    "Choosing the right tokenizer is crucial for building robust and high-performing NLP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T15:19:12.937043Z",
     "start_time": "2024-03-30T15:19:08.502298Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T15:19:12.982355Z",
     "start_time": "2024-03-30T15:19:12.943334Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['don', \"'\", 't', 'be', 'so', 'judgment', '##al']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"don't be so judgmental\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T15:19:13.013763Z",
     "start_time": "2024-03-30T15:19:12.985462Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1274, 112, 189, 1129, 1177, 9228, 1348]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_text_ids = tokenizer.convert_tokens_to_ids(tokenizer.tokenize(\"don't be so judgmental\"))\n",
    "tokenized_text_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T15:19:17.098703Z",
     "start_time": "2024-03-30T15:19:13.015833Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"don't be so judgmental\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = tokenizer.decode(tokenized_text_ids)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T15:19:17.129894Z",
     "start_time": "2024-03-30T15:19:17.103793Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 1274,  112,  189, 1129, 1177, 9228, 1348,  102]])\n",
      "[CLS] don't be so judgmental [SEP]\n",
      "don't be so judgmental\n"
     ]
    }
   ],
   "source": [
    "t = tokenizer.encode(\"don't be so judgmental\", return_tensors='pt')  # tokenizer will return pytorch tensors\n",
    "\n",
    "print(t)\n",
    "print(tokenizer.decode(t[0]))  # print decoded string with special tokens included\n",
    "print(tokenizer.decode(t[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### encode() vs encode_plus() methods\n",
    "\n",
    ".encode(text):\n",
    "\n",
    "- This method simply converts the input text into token IDs.\n",
    "- It returns a list of token IDs representing the input text.\n",
    "- This method is straightforward and useful when you only need token IDs for the input.\n",
    "\n",
    ".encode_plus(text, ...):\n",
    "\n",
    "- In addition to converting the input text into token IDs, this method also generates additional information such as attention masks, token type IDs, etc., depending on the specific model and tokenizer.\n",
    "- It returns a dictionary containing token IDs ('input_ids'), attention mask ('attention_mask'), and potentially other information like token type IDs ('token_type_ids'), depending on the model architecture.\n",
    "- This method is more versatile and useful when you need additional information along with token IDs, such as when preparing inputs for model training or inference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T15:19:17.166848Z",
     "start_time": "2024-03-30T15:19:17.136138Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1274,  112,  189, 1129, 1177, 9228, 1348,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode_plus(\"don't be so judgmental\", return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "If we have more than one sequence, we use batch encode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T15:19:17.199007Z",
     "start_time": "2024-03-30T15:19:17.169072Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[101, 1274, 112, 189, 1129, 1177, 9228, 1348, 102], [101, 178, 1821, 170, 2377, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_encode_plus([\"don't be so judgmental\", 'i am a student'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We usually want that all sequences in a batch are of the same length. Therefor we need to decide how to prepare them. If we set padding to True the tokenizer will pad to the longest sequence in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T15:19:17.229836Z",
     "start_time": "2024-03-30T15:19:17.203274Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1274,  112,  189, 1129, 1177, 9228, 1348,  102],\n",
       "        [ 101,  178, 1821,  170, 2377,  102,    0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0]])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_encode_plus([\"don't be so judgmental\", 'i am a student'], padding=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T15:19:17.260188Z",
     "start_time": "2024-03-30T15:19:17.230895Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1274,  112,  189, 1129, 1177, 9228, 1348,  102,    0,    0,    0,\n",
       "            0,    0,    0],\n",
       "        [ 101,  178, 1821,  170, 2377,  102,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.batch_encode_plus([\"don't be so judgmental\", 'i am a student'], padding='max_length', max_length=15, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T15:19:17.307486Z",
     "start_time": "2024-03-30T15:19:17.262751Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 146, 12765, 146, 6586, 140, 19556, 19368, 13329, 118, 162, 21678, 2162, 17056, 1121, 1139, 1888, 2984, 1272, 1104, 1155, 1103, 6392, 1115, 4405, 1122, 1165, 1122, 1108, 1148, 1308, 1107, 2573, 119, 146, 1145, 1767, 1115, 1120, 1148, 1122, 1108, 7842, 1118, 158, 119, 156, 119, 10148, 1191, 1122, 1518, 1793, 1106, 3873, 1142, 1583, 117, 3335, 1217, 170, 5442, 1104, 2441, 1737, 107, 6241, 107, 146, 1541, 1125, 1106, 1267, 1142, 1111, 1991, 119, 133, 9304, 120, 135, 133, 9304, 120, 135, 1109, 4928, 1110, 8663, 1213, 170, 1685, 3619, 3362, 2377, 1417, 14960, 1150, 3349, 1106, 3858, 1917, 1131, 1169, 1164, 1297, 119, 1130, 2440, 1131, 3349, 1106, 2817, 1123, 2209, 1116, 1106, 1543, 1199, 3271, 1104, 4148, 1113, 1184, 1103, 1903, 156, 11547, 1162, 1354, 1164, 2218, 1741, 2492, 1216, 1112, 1103, 4357, 1414, 1105, 1886, 2492, 1107, 1103, 1244, 1311, 119, 1130, 1206, 4107, 8673, 1105, 6655, 10552, 3708, 2316, 1104, 8583, 1164, 1147, 11089, 1113, 4039, 117, 1131, 1144, 2673, 1114, 1123, 3362, 3218, 117, 22150, 117, 1105, 1597, 1441, 119, 133, 9304, 120, 135, 133, 9304, 120, 135, 1327, 8567, 1143, 1164, 146, 6586, 140, 19556, 19368, 13329, 118, 162, 21678, 2162, 17056, 1110, 1115, 1969, 1201, 2403, 117, 1142, 1108, 1737, 185, 8456, 9597, 119, 8762, 117, 1103, 2673, 1105, 183, 17294, 2340, 4429, 1132, 1374, 1105, 1677, 1206, 117, 1256, 1173, 1122, 112, 188, 1136, 2046, 1176, 1199, 10928, 1193, 1189, 185, 8456, 1186, 119, 1799, 1139, 1583, 2354, 1713, 1525, 1122, 19196, 117, 1107, 3958, 2673, 1105, 183, 17294, 2340, 1132, 170, 1558, 22088, 1107, 3619, 7678, 119, 2431, 1130, 14721, 1197, 27644, 117, 18271, 1147, 2590, 1106, 1363, 1385, 2298, 1287, 4100, 117, 1125, 2673, 4429, 1107, 1117, 2441, 119, 133, 9304, 120, 135, 133, 9304, 120, 135, 146, 1202, 3254, 2354, 1181, 1103, 18992, 1111, 1103, 1864, 1115, 1251, 2673, 2602, 1107, 1103, 1273, 1110, 2602, 1111, 6037, 4998, 1897, 1190, 1198, 1106, 4900, 1234, 1105, 1294, 1948, 1106, 1129, 2602, 1107, 185, 8456, 9597, 13090, 1107, 1738, 119, 146, 6586, 140, 19556, 19368, 13329, 118, 162, 21678, 2162, 17056, 1110, 170, 1363, 1273, 1111, 2256, 5277, 1106, 2025, 1103, 6092, 1105, 15866, 113, 1185, 23609, 1179, 3005, 114, 1104, 3619, 7678, 119, 1252, 1541, 117, 1142, 1273, 2144, 112, 189, 1138, 1277, 1104, 170, 4928, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenize first sample in the train set\n",
    "tokenizer(imdb['train'][0]['text'], padding='max_length', max_length=tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Lets tokenize now the whole dataset. We will set truncation to True: Truncate to a maximum length specified with the argument max_length or to the maximum acceptable input length for the model if that argument is not provided. This will truncate token by token, removing a token from the longest sequence in the pair if a pair of sequences (or a batch of pairs) is provided.\n",
    "\n",
    "We will also use .map method on the dataset. Lets emphasize here that the cache is one of the reasons why ðŸ¤— Datasets is so efficient. It stores previously downloaded and processed datasets so when you need to use them again, they are reloaded directly from the cache. This avoids having to download a dataset all over again, or reapplying processing functions. Even after you close and start another Python session, ðŸ¤— Datasets will reload your dataset directly from the cache!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-03-30T15:19:17.339262Z",
     "start_time": "2024-03-30T15:19:17.309470Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define preprocess function\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Combining the utility of Dataset.map() with batch mode is very powerful. It allows you to speed up processing, and freely control the size of the generated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-03-30T15:19:17.340275Z"
    },
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a1e4130a7d34c5a959b693e0d55a6b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d61f48bc4d84c928e1d0d6d4ac45a8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e7fd2a4a86740789538e3b10b9f7762",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize dataset\n",
    "tokenized_imdb = imdb.map(preprocess_function, batched=True, batch_size=1000, load_from_cache_file=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "tokenized_imdb['train'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "len(tokenized_imdb['train'][10]['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Data collators are objects that will form a batch by using a list of dataset elements as input. These elements are of the same type as the elements of train_dataset or eval_dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We also need an evaluation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Model \n",
    "\n",
    "When loading the pre-trained BertForSequenceClassification model from the bert-base-cased checkpoint, it's important to note that certain weights, specifically those associated with the classifier layer ('classifier.bias' and 'classifier.weight'), are not initialized from the checkpoint. This occurs because the BertForSequenceClassification model adapts the base BERT model for a specific sequence classification task, which often requires a custom final classifier layer tailored to the number of classes in the specific task at hand.\n",
    "\n",
    "NOTE: This part of the notebook requires a lot of compute resources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
    "label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./runs\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_imdb[\"train\"],\n",
    "    eval_dataset=tokenized_imdb[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your model\n",
    "trainer.save_model('./models/sentiment-bert')\n",
    "\n",
    "# Load your model\n",
    "model = BertForSequenceClassification.from_pretrained('./models/sentiment-bert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text = \"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\", model=\"./models/sentiment-bert\")\n",
    "classifier(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom neural model for IMDB reviews sentiment prediction\n",
    "\n",
    "## Key Components\n",
    "\n",
    "1. `LightningCustomIMDBModel`: A PyTorch Lightning module that defines the model architecture, training step, validation step, and optimizer configuration.\n",
    "2. `IMDBDataset`: A custom PyTorch dataset class to handle tokenized IMDb reviews, ensuring they are correctly batched and passed to the model.\n",
    "3. Training and validation loop setup using PyTorch Lightning's `Trainer`, with added functionality for model checkpointing and early stopping to prevent overfitting.\n",
    "4. Demonstration of model inference on new data, showcasing the model's ability to evaluate sentiment on unseen movie reviews.\n",
    "\n",
    "NOTE: This part of the notebook requires a lot of compute resources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, BertTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LightningCustomIMDBModel(L.LightningModule):\n",
    "\n",
    "    def __init__(self, vocabulary_size, embedding_dimensions=128, cnn_filters=50, dnn_units=512, model_output_classes=2,\n",
    "                 dropout_rate=0.1, learning_rate=1e-4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_output_classes = model_output_classes\n",
    "        self.embedding = nn.Embedding(vocabulary_size, embedding_dimensions)\n",
    "        self.cnn_layer1 = nn.Conv1d(embedding_dimensions, cnn_filters, kernel_size=2)\n",
    "        self.cnn_layer2 = nn.Conv1d(embedding_dimensions, cnn_filters, kernel_size=3)\n",
    "        self.cnn_layer3 = nn.Conv1d(embedding_dimensions, cnn_filters, kernel_size=4)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense_1 = nn.Linear(cnn_filters * 3, dnn_units)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        if self.model_output_classes == 2:\n",
    "            self.last_dense = nn.Linear(dnn_units, 1)\n",
    "            self.activation = torch.sigmoid\n",
    "        else:\n",
    "            self.last_dense = nn.Linear(dnn_units, model_output_classes)\n",
    "            self.activation = F.softmax\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, input_ids, labels=None):\n",
    "        x = self.embedding(input_ids).permute(0, 2, 1)\n",
    "        x1 = self.pool(F.relu(self.cnn_layer1(x)))\n",
    "        x2 = self.pool(F.relu(self.cnn_layer2(x)))\n",
    "        x3 = self.pool(F.relu(self.cnn_layer3(x)))\n",
    "\n",
    "        concatenated = self.flatten(torch.cat((x1, x2, x3), dim=1))\n",
    "        concatenated = F.relu(self.dense_1(concatenated))\n",
    "        concatenated = self.dropout(concatenated)\n",
    "\n",
    "        logits = self.last_dense(concatenated)\n",
    "\n",
    "        outputs = {'logits': logits}\n",
    "        if labels is not None:\n",
    "            if self.model_output_classes == 2:  # Binary classification\n",
    "                loss_fct = nn.BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1).type_as(logits))\n",
    "            else:  # Multiclass classification\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.model_output_classes), labels.view(-1))\n",
    "            outputs['loss'] = loss\n",
    "            self.log(\"my_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return outputs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Here, you define one training step\n",
    "        input_ids, labels = batch['input_ids'], batch['labels']\n",
    "        outputs = self(input_ids, labels)\n",
    "        loss = outputs['loss']\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, labels = batch['input_ids'], batch['labels']\n",
    "        outputs = self(input_ids, labels)\n",
    "        val_loss = outputs['loss']\n",
    "\n",
    "        logits = outputs['logits']\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "        # Determine class predictions with a threshold of 0.5\n",
    "        preds = (probs >= 0.5).astype(int).flatten()\n",
    "\n",
    "        # Ensure labels are on the same device as preds and also flattened\n",
    "        labels = labels.cpu().flatten().numpy().astype(int)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        correct = np.sum(preds == labels)  # Count how many predictions match the labels\n",
    "        total = len(labels)  # Total number of labels\n",
    "        acc = correct / total  # Calculate the accuracy\n",
    "\n",
    "        # Log validation loss and accuracy\n",
    "        self.log('val_loss', val_loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "\n",
    "        # Return the loss and accuracy\n",
    "        return {'val_loss': val_loss, 'val_acc': acc}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids, labels = batch['input_ids'], batch['labels']\n",
    "        outputs = self(input_ids, labels)\n",
    "        val_loss = outputs['loss']\n",
    "\n",
    "        logits = outputs['logits']\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "        # Determine class predictions with a threshold of 0.5\n",
    "        preds = (probs >= 0.5).astype(int).flatten()\n",
    "\n",
    "        # Ensure labels are on the same device as preds and also flattened\n",
    "        labels = labels.cpu().flatten().numpy().astype(int)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        correct = np.sum(preds == labels)  # Count how many predictions match the labels\n",
    "        total = len(labels)   # Total number of labels\n",
    "        acc = correct / total  # Calculate the accuracy\n",
    "\n",
    "        # Log validation loss and accuracy\n",
    "        self.log('test_loss', val_loss, prog_bar=True)\n",
    "        self.log('test_acc', acc, prog_bar=True)\n",
    "\n",
    "        # Return the loss and accuracy\n",
    "        return {'test_loss': val_loss, 'test_acc': acc}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Define optimizer (and scheduler if necessary)\n",
    "        optimizer = AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "imdb = load_dataset(\"imdb\")\n",
    "del imdb['unsupervised']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 256\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenized_imdb = imdb.map(lambda examples: tokenizer(examples['text'], truncation=True, padding='max_length', max_length=MAX_SEQ_LENGTH), batched=True)\n",
    "\n",
    "# Split the train dataset into train and validation\n",
    "train_test_split = tokenized_imdb[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "# Create a DatasetDict to hold the split datasets\n",
    "split_datasets = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'validation': train_test_split['test'],\n",
    "    'test': tokenized_imdb['test']\n",
    "})\n",
    "\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, tokenized_dataset):\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.tokenized_dataset[idx]\n",
    "        return {\"input_ids\": torch.tensor(item['input_ids'], dtype=torch.long),\n",
    "                \"labels\": torch.tensor(item['label'], dtype=torch.float if OUTPUT_CLASSES == 2 else torch.long)}\n",
    "\n",
    "\n",
    "BATCH_SIZE = 512\n",
    "train_dataset = IMDBDataset(split_datasets[\"train\"])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "val_dataset = IMDBDataset(split_datasets[\"validation\"])\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "test_dataset = IMDBDataset(split_datasets[\"test\"])\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "VOCAB_LENGTH = len(tokenizer.vocab)\n",
    "EMB_DIM = 200\n",
    "CNN_FILTERS = 100\n",
    "DNN_UNITS = 256\n",
    "OUTPUT_CLASSES = 2\n",
    "DROPOUT_RATE = 0.2\n",
    "NB_EPOCHS = 15\n",
    "\n",
    "# Instantiate the model\n",
    "model = LightningCustomIMDBModel(VOCAB_LENGTH, EMB_DIM, CNN_FILTERS, DNN_UNITS, OUTPUT_CLASSES, DROPOUT_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To save a checkpoint automatically during training, you can use callbacks like ModelCheckpoint\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Instantiate built-in callbacks (optional)\n",
    "checkpoint_callback = ModelCheckpoint(dirpath='checkpoints/', save_top_k=1, verbose=True, monitor='train_loss', mode='min')\n",
    "early_stopping_callback = EarlyStopping(monitor='train_loss', patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "trainer = L.Trainer(callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "                    max_epochs=NB_EPOCHS,\n",
    "                    accelerator='gpu',\n",
    "                    devices=1,\n",
    "                    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, train_dataloader, val_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Test the model\n",
    "trainer.test(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "Model will output something like this:\n",
    "\n",
    "```plaintext\n",
    "/home/azagar/.miniconda3/bin/conda run -n whisper2 --no-capture-output python /home/azagar/myfiles/custom_bert/custom_model_lightning.py \n",
    "GPU available: True (cuda), used: True\n",
    "TPU available: False, using: 0 TPU cores\n",
    "IPU available: False, using: 0 IPUs\n",
    "HPU available: False, using: 0 HPUs\n",
    ".... \n",
    "/home/azagar/local/miniconda3/envs/whisper2/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/logger_connector/result.py:441: It is recommended to use `self.log('test_acc', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
    "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:02<00:00, 11.27it/s]\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "       Test metric             DataLoader 0\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        test_acc            0.7319414615631104\n",
    "        test_loss           0.5770595073699951\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "Process finished with exit code 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new = 'I don\\'t like this movie.'\n",
    "inps = tokenizer(new, truncation=True, padding='max_length', max_length=MAX_SEQ_LENGTH, return_tensors='pt')['input_ids'].to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(torch.sigmoid(model(inps)['logits']))\n",
    "\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "model.load_from_checkpoint(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# To load the model:\n",
    "model = LightningCustomIMDBModel.load_from_checkpoint('./checkpoints/*.ckpt')\n",
    "model.eval()\n",
    "new = 'I don\\'t like this movie.'\n",
    "inps = tokenizer(new, truncation=True, padding='max_length', max_length=MAX_SEQ_LENGTH, return_tensors='pt')['input_ids'].to('cuda')\n",
    "\n",
    "with torch.no_grad():\n",
    "    print(torch.sigmoid(model(inps)['logits']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom neural model for IMDB reviews sentiment prediction using BERT Embeddings\n",
    "\n",
    "## Key Highlights:\n",
    "\n",
    "- **BERT Embeddings:** We utilize embeddings from a pre-trained BERT model as the foundation for our feature extraction. BERT's deep understanding of language semantics, garnered from extensive pre-training on diverse corpora, provides a rich contextual basis for our sentiment analysis task.\n",
    "\n",
    "- **Freezing Weights:** To preserve the intrinsic language understanding capabilities of BERT and expedite training, we freeze the weights of the pre-trained layers. This approach allows us to benefit from BERT's pre-trained knowledge without the computational overhead of fine-tuning millions of parameters.\n",
    "\n",
    "NOTE: This part of the notebook requires a lot of compute resources. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, BertTokenizer, AutoModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class LightningCustomIMDBModel(L.LightningModule):\n",
    "\n",
    "    def __init__(self, model_name, cnn_filters=50, dnn_units=512, model_output_classes=2,\n",
    "                 dropout_rate=0.1, learning_rate=1e-4, freeze=True):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model_output_classes = model_output_classes\n",
    "        self.bert = AutoModel.from_pretrained(model_name)  # Load pre-trained BERT\n",
    "        self.bert.train()  # The model is by default in eval mode\n",
    "\n",
    "        # Freeze BERT parameters\n",
    "        if freeze:\n",
    "            self.bert.eval()\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        embedding_dimensions = self.bert.config.hidden_size  # Use the embedding size from BERT config\n",
    "\n",
    "        self.cnn_layer1 = nn.Conv1d(embedding_dimensions, cnn_filters, kernel_size=2)\n",
    "        self.cnn_layer2 = nn.Conv1d(embedding_dimensions, cnn_filters, kernel_size=3)\n",
    "        self.cnn_layer3 = nn.Conv1d(embedding_dimensions, cnn_filters, kernel_size=4)\n",
    "        self.pool = nn.AdaptiveMaxPool1d(1)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense_1 = nn.Linear(cnn_filters * 3, dnn_units)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        if self.model_output_classes == 2:\n",
    "            self.last_dense = nn.Linear(dnn_units, 1)\n",
    "            self.activation = torch.sigmoid\n",
    "        else:\n",
    "            self.last_dense = nn.Linear(dnn_units, model_output_classes)\n",
    "            self.activation = F.softmax\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, labels=None):\n",
    "        # Get embeddings from BERT\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Use the last hidden state as embeddings\n",
    "        embeddings = bert_output.last_hidden_state.permute(0, 2, 1)  # Permute to match (batch_size, channels, length)\n",
    "\n",
    "        x1 = self.pool(F.relu(self.cnn_layer1(embeddings)))\n",
    "        x2 = self.pool(F.relu(self.cnn_layer2(embeddings)))\n",
    "        x3 = self.pool(F.relu(self.cnn_layer3(embeddings)))\n",
    "\n",
    "        concatenated = self.flatten(torch.cat((x1, x2, x3), dim=1))\n",
    "        concatenated = F.relu(self.dense_1(concatenated))\n",
    "        concatenated = self.dropout(concatenated)\n",
    "\n",
    "        logits = self.last_dense(concatenated)\n",
    "\n",
    "        outputs = {'logits': logits}\n",
    "        if labels is not None:\n",
    "            if self.model_output_classes == 2:  # Binary classification\n",
    "                loss_fct = nn.BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits.view(-1), labels.view(-1).type_as(logits))\n",
    "            else:  # Multiclass classification\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.model_output_classes), labels.view(-1))\n",
    "            outputs['loss'] = loss\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # Here, you define one training step\n",
    "        input_ids, attention_mask, labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "        outputs = self(input_ids, attention_mask, labels)\n",
    "        loss = outputs['loss']\n",
    "        self.log(\"train_loss\", loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "        outputs = self(input_ids, attention_mask, labels)\n",
    "        val_loss = outputs['loss']\n",
    "\n",
    "        logits = outputs['logits']\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "        # Determine class predictions with a threshold of 0.5\n",
    "        preds = (probs >= 0.5).astype(int).flatten()\n",
    "\n",
    "        # Ensure labels are on the same device as preds and also flattened\n",
    "        labels = labels.cpu().flatten().numpy().astype(int)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        correct = np.sum(preds == labels)  # Count how many predictions match the labels\n",
    "        total = len(labels)  # Total number of labels\n",
    "        acc = correct / total  # Calculate the accuracy\n",
    "\n",
    "        # Log validation loss and accuracy\n",
    "        self.log('val_loss', val_loss, prog_bar=True)\n",
    "        self.log('val_acc', acc, prog_bar=True)\n",
    "\n",
    "        # Return the loss and accuracy\n",
    "        return {'val_loss': val_loss, 'val_acc': acc}\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        input_ids, attention_mask, labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n",
    "        outputs = self(input_ids, attention_mask, labels)\n",
    "        val_loss = outputs['loss']\n",
    "\n",
    "        logits = outputs['logits']\n",
    "        probs = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "        # Determine class predictions with a threshold of 0.5\n",
    "        preds = (probs >= 0.5).astype(int).flatten()\n",
    "\n",
    "        # Ensure labels are on the same device as preds and also flattened\n",
    "        labels = labels.cpu().flatten().numpy().astype(int)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        correct = np.sum(preds == labels)  # Count how many predictions match the labels\n",
    "        total = len(labels)  # Total number of labels\n",
    "        acc = correct / total  # Calculate the accuracy\n",
    "\n",
    "        # Log validation loss and accuracy\n",
    "        self.log('test_loss', val_loss, prog_bar=True)\n",
    "        self.log('test_acc', acc, prog_bar=True)\n",
    "\n",
    "        # Return the loss and accuracy\n",
    "        return {'test_loss': val_loss, 'test_acc': acc}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # Define optimizer (and scheduler if necessary)\n",
    "        optimizer = AdamW(self.parameters(), lr=self.learning_rate)\n",
    "        return optimizer\n",
    "\n",
    "\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "\n",
    "imdb = load_dataset(\"imdb\")\n",
    "\n",
    "del imdb['unsupervised']\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenized_imdb = imdb.map(lambda examples: tokenizer(examples['text'], padding='max_length', truncation=True, max_length=256, return_tensors='pt', return_attention_mask=True), batched=True)\n",
    "\n",
    "# Split the train dataset into train and validation\n",
    "train_test_split = tokenized_imdb[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "# Create a DatasetDict to hold the split datasets\n",
    "split_datasets = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'validation': train_test_split['test'],\n",
    "    'test': tokenized_imdb['test']\n",
    "})\n",
    "\n",
    "\n",
    "class IMDBDataset(Dataset):\n",
    "    def __init__(self, tokenized_dataset):\n",
    "        self.tokenized_dataset = tokenized_dataset\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokenized_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.tokenized_dataset[idx]\n",
    "        return {\"input_ids\": torch.tensor(item['input_ids'], dtype=torch.long),\n",
    "                \"attention_mask\": torch.tensor(item['attention_mask'], dtype=torch.long),\n",
    "                \"labels\": torch.tensor(item['label'], dtype=torch.float if OUTPUT_CLASSES == 2 else torch.long)}\n",
    "\n",
    "\n",
    "BATCH_SIZE = 16\n",
    "train_dataset = IMDBDataset(split_datasets[\"train\"])\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=20)\n",
    "\n",
    "val_dataset = IMDBDataset(split_datasets[\"validation\"])\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, num_workers=20)\n",
    "\n",
    "test_dataset = IMDBDataset(split_datasets[\"test\"])\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, num_workers=20)\n",
    "\n",
    "\n",
    "MODEL_NAME = 'bert-base-uncased'\n",
    "CNN_FILTERS = 100\n",
    "DNN_UNITS = 256\n",
    "OUTPUT_CLASSES = 2\n",
    "DROPOUT_RATE = 0.2\n",
    "NB_EPOCHS = 5\n",
    "FREEZE = True\n",
    "\n",
    "# To save a checkpoint automatically during training, you can use callbacks like ModelCheckpoint\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Instantiate the model\n",
    "model = LightningCustomIMDBModel(MODEL_NAME, CNN_FILTERS, DNN_UNITS, OUTPUT_CLASSES, DROPOUT_RATE, freeze=FREEZE)\n",
    "\n",
    "# Instantiate built-in callbacks (optional)\n",
    "checkpoint_callback = ModelCheckpoint(dirpath='checkpoints/', save_top_k=1, verbose=True, monitor='train_loss', mode='min')\n",
    "early_stopping_callback = EarlyStopping(monitor='train_loss', patience=3)\n",
    "\n",
    "\n",
    "trainer = L.Trainer(callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "                    max_epochs=NB_EPOCHS,\n",
    "                    accelerator='gpu',\n",
    "                    devices=3,\n",
    "                    log_every_n_steps=10,\n",
    "                    strategy='ddp_find_unused_parameters_true'  # For training on multiple gpus\n",
    "                    )\n",
    "\n",
    "# Train the model\n",
    "trainer.fit(model, train_dataloader, val_dataloader)\n",
    "\n",
    "# Test the model\n",
    "trainer.test(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "```plaintext\n",
    "Model will output something like this:\n",
    "\n",
    "GPU available: True (cuda), used: True\n",
    "TPU available: False, using: 0 TPU cores\n",
    "IPU available: False, using: 0 IPUs\n",
    "HPU available: False, using: 0 HPUs\n",
    "\n",
    "....\n",
    "Epoch 4: 100%|â–ˆ| 176/176 [02:08<00:00,  1.37it/s, v_num=32, val_loss=0.465, val_\n",
    "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
    "Testing DataLoader 0: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 196/196 [02:01<00:00,  1.61it/s]\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "       Test metric             DataLoader 0\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        test_acc            0.7701200246810913\n",
    "        test_loss           0.47308972477912903\n",
    "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "Process finished with exit code 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT Text Classification IMDB.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
